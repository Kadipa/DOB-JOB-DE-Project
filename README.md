# ğŸ™ï¸ NYC DOB Job Data Pipeline â€” DE Zoomcamp 2025 Course Project

This project is part of the Data Engineering Zoomcamp course and follows the full project rubric. It builds a real-world batch data pipeline to ingest, transform, and visualize New York City Department of Buildings (DOB) job application filings using modern data engineering tools and AWS cloud services.

---

## ğŸ¯ Problem Description

New York Cityâ€™s Department of Buildings publishes building job application data. This project builds a pipeline that:

- Automates ingestion and transformation of raw job data
- Data load from the API by using dlt(data load tool)
- Stores it in an AWS-based lakehouse architecture (S3 + Redshift)
- Transforms the data using dbt
- Visualizes insights on job types

The goal: enable stakeholders to explore trends in construction filings via an easy-to-use dashboard.

---

## ğŸ“š Dataset & Design Decisions

### ğŸ“ Dataset Source

This project uses the [NYC Job Application Filings dataset](https://data.cityofnewyork.us/City-Government/NYC-Jobs/pda4-rgn4/about_data) published by the City of New York.

- It contains detailed information about building job filings in NYC
- The data is **updated daily**, making it suitable for periodic ingestion
- Data is retrieved via a REST API and stored in Parquet format

---

### â³ Why Batch Processing?

Since the dataset is updated daily (not in real-time), a **batch processing** architecture was chosen:

- âœ… Ingestion is triggered **once per day**
- âœ… Keeps infrastructure cost and complexity low
- âœ… Ideal for scheduled workflows using **Airflow**
- âœ… Supports dbt transformations and downstream analytics without needing stream processors

---

## â˜ï¸ Cloud Setup

- All services are deployed using **AWS**:
  - Redshift Serverless (data warehouse)
  - S3 (data lake)
  - AWS Glue (catalog)
  - IAM Roles for permissioning
- Infrastructure provisioned using **Terraform** as IaC(Infrastructure as code)

---

## ğŸ“¦ Pipeline Overview (Batch)

This is a **batch pipeline**, scheduled to run daily via Airflow.

### Pipeline Steps:

1. **DLT Ingestion**: Calls NYC DOB Job API and stores data as Parquet in local FS
2. **S3 Upload**: Moves data to S3
3. **Glue Crawler**: Catalogs data into AWS Glue Data Catalog
4. **Redshift Spectrum**: Creates external table from Glue catalog
5. **Internal Copy**: Copies raw data from Spectrum into Redshift internal table
6. **dbt Transform**: Cleans, models, and tests data
7. **Metabase Dashboard**: Loads data for business visualization

Orchestrated via **Airflow DAG** with all tasks automated.

---

## ğŸ§° Tools & Tech

| Layer                | Tool                        |
|---------------------|-----------------------------|
| Ingestion           | DLT                         |
| Data Lake           | AWS S3                      |
| Catalog             | AWS Glue                    |
| DWH                 | Redshift + Spectrum         |
| Transformations     | dbt                         |
| Orchestration       | Apache Airflow              |
| Visualization       | Metabase                    |
| IaC                 | Terraform                   |
| Language            | Python                      |
| Containerization    | Docker + docker-compose     |

---

## ğŸ§ª dbt Transformations

- `stg_dob_jobs.sql`: staging model from raw data
- `cleaned_jobs.sql`: final analytics model
- Uses `dbt test` to validate schema and nulls
- Materialized as tables in `analytics` schema

---

## ğŸ“Š Dashboard (Metabase)

Dashboard accessible via Metabase with:

![Dashboard](./Data-Dashboard/dash-board.gif)

We can also see how the data is passed to the metabase, tables and job summary.

![Database](./Data-Dashboard/database.png)

Fig1: Database in the metabase

![Analytics_table](./Data-Dashboard/analytics_table.png)

Fig 2: Analytics Table

![Job_Summary](./Data-Dashboard/jobSummary.png)

Fig 3: Job Summary 

---

## ğŸ› ï¸ Infrastructure with Terraform

Provisioned using `terraform/`:

- Redshift Serverless Workgroup
- IAM Role for Redshift Spectrum
- Glue Database + Crawler
- S3 Path & Permissions

State files are excluded via `.gitignore`.

## ğŸ“ Project Structure

```plaintext
DOB-JOB-DE-PROJECT/
â”œâ”€â”€ dags/                         # Airflow DAGs and dbt project
â”‚   â”œâ”€â”€ dob_dbt_project/          # dbt transformations
â”‚   â”‚   â”œâ”€â”€ analyses/
â”‚   â”‚   â”œâ”€â”€ macros/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ seeds/
â”‚   â”‚   â”œâ”€â”€ snapshots/
â”‚   â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”‚   â”œâ”€â”€ profiles.yml
â”‚   â”‚   â””â”€â”€ .gitignore
â”‚   â””â”€â”€ full_pipeline_dag.py      # Airflow DAG to run the pipeline
â”‚
â”œâ”€â”€ job-dlt-pipeline/             # DLT ingestion pipeline
â”‚   â”œâ”€â”€ .dlt/
â”‚   â”‚   â”œâ”€â”€ config.toml    # (Should be gitignored)
â”‚   â”‚   â””â”€â”€ secrets.toml   # (Should be gitignored)
â”‚   â”œâ”€â”€ rest_api_pipeline.py
â”‚   â””â”€â”€ .gitignore
â”‚
â”œâ”€â”€ scripts/                      # Custom scripts for AWS, Redshift, etc.
â”‚   â”œâ”€â”€ copy_to_redshift.py
â”‚   â”œâ”€â”€ create_external_schema.py
â”‚   â”œâ”€â”€ metabase_automation.py
â”‚   â””â”€â”€ run_glue_crawler.py
â”‚
â”œâ”€â”€ terraform/                    # Infrastructure as Code (Terraform)
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â”œâ”€â”€ outputs.tf
â”‚   â”œâ”€â”€ terraform.tfvars          # (Should be gitignored)
â”‚   â”œâ”€â”€ terraform.tfstate         
â”‚   â”œâ”€â”€ .terraform/               # (Should be gitignored)
â”‚   â”œâ”€â”€ .terraform.lock.hcl
â”‚   â””â”€â”€ .gitignore
â”‚
â”œâ”€â”€ Dockerfile                    # Custom Airflow image
â”œâ”€â”€ docker-compose.yaml           # Run Airflow + Metabase
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .env                          # Actual secrets (excluded from Git)
â”œâ”€â”€ .env.example                  # Example env file
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md                     # Project documentation
â””â”€â”€ Data-Dashboard/
    â””â”€â”€ nyc_dob_dashboard.png     # Image for README preview
```
---

## ğŸ’» Reproducibility

### 1. Clone this repo & setup env

```
git clone https://github.com/YOUR_USERNAME/nyc-dob-job-pipeline.git
cd nyc-dob-job-pipeline
```
Please create .env file and fill in(Please check the .envExample):

- AWS keys
- Redshift credentials
- IAM role
- Metabase user
- dlt secret.toml

### 2. Provision AWS Infrastructure Using Terraform
Terraform will create all required AWS resources automatically:

```bash
cd terraform/
terraform init
terraform apply
 ```

Terraform will create:

- S3 bucket for storing raw data
- Redshift Serverless workgroup
- IAM role for Redshift Spectrum

- After completion, copy any outputs (bucket name, role ARN, etc.) into your .env(Please check .envExample).

### 3. Manually Set DLT Secrets 

```
# job-dlt-pipeline/.dlt/secrets.toml
[default]
aws_access_key_id = "YOUR_AWS_ACCESS_KEY"
aws_secret_access_key = "YOUR_AWS_SECRET_KEY"
```

- These are required to allow DLT to upload to S3.

### 4. Run containers

From the project root:

```docker-compose up --build```

âœ… Airflow: http://localhost:8080

âœ… Metabase: http://localhost:3000

Trigger DAG in Airflow to run full pipeline.

### Bonus Improvements

âœ… Airflow DAG with clear dependencies

âœ… Automated Metabase dashboard creation

âœ… Modular repo with separate folders per tool

âœ… .gitignore for security


### ğŸ›  Redshift VARCHAR Length Fix Issue

During ingestion from Redshift Spectrum to internal Redshift tables, the pipeline encountered the following error:

``` psycopg2.errors.InternalError_: Value too long for type character varying(256) ```

#### âœ… Fix:

- All columns were **trimmed using `LEFT(column, 256)`** during the SELECT phase
- The `job_description` column â€” which contains long text â€” was preserved using **`VARCHAR(65535)`** in the Redshift table schema
- This ensured compatibility with Redshift without losing important descriptive information

---

These decisions were made to ensure that the pipeline is scalable, cost-efficient, and robust for daily update.



