# ğŸ™ï¸ NYC DOB Job Data Pipeline â€” DE Zoomcamp 2025 Course Project

This project is part of the Data Engineering Zoomcamp course and follows the full project rubric. It builds a real-world batch data pipeline to ingest, transform, and visualize New York City Department of Buildings (DOB) job application filings using modern data engineering tools and AWS cloud services.

---

## ğŸ¯ Problem Description

New York Cityâ€™s Department of Buildings publishes building job application data. This project builds a pipeline that:

- Automates ingestion and transformation of raw job data
- Data load from the API by using dlt(data load tool)
- Stores it in an AWS-based lakehouse architecture (S3 + Redshift)
- Transforms the data using dbt
- Visualizes insights on job types, e-filing, equipment usage, and building types

The goal: enable stakeholders to explore trends in construction filings via an easy-to-use dashboard.

---

## â˜ï¸ Cloud Setup

- All services are deployed using **AWS**:
  - Redshift Serverless (data warehouse)
  - S3 (data lake)
  - AWS Glue (catalog)
  - IAM Roles for permissioning
- Infrastructure provisioned using **Terraform** as IaC(Infrastructure as code)

---

## ğŸ“¦ Pipeline Overview (Batch)

This is a **batch pipeline**, scheduled to run daily via Airflow.

### Pipeline Steps:

1. **DLT Ingestion**: Calls NYC DOB Job API and stores data as Parquet in local FS
2. **S3 Upload**: Moves data to S3
3. **Glue Crawler**: Catalogs data into AWS Glue Data Catalog
4. **Redshift Spectrum**: Creates external table from Glue catalog
5. **Internal Copy**: Copies raw data from Spectrum into Redshift internal table
6. **dbt Transform**: Cleans, models, and tests data
7. **Metabase Dashboard**: Loads data for business visualization

Orchestrated via **Airflow DAG** with all tasks automated.

---

## ğŸ§° Tools & Tech

| Layer                | Tool                        |
|---------------------|-----------------------------|
| Ingestion           | DLT                         |
| Data Lake           | AWS S3                      |
| Catalog             | AWS Glue                    |
| DWH                 | Redshift + Spectrum         |
| Transformations     | dbt                         |
| Orchestration       | Apache Airflow              |
| Visualization       | Metabase                    |
| IaC                 | Terraform                   |
| Language            | Python                      |
| Containerization    | Docker + docker-compose     |

---

## ğŸ§ª dbt Transformations

- `stg_dob_jobs.sql`: staging model from raw data
- `cleaned_jobs.sql`: final analytics model
- Uses `dbt test` to validate schema and nulls
- Materialized as tables in `analytics` schema

---

## ğŸ“Š Dashboard (Metabase)

Dashboard accessible via Metabase with:


![Dashboard](./Data-Dashbord/dash-board.mov)

---

## ğŸ› ï¸ Infrastructure with Terraform

Provisioned using `terraform/`:

- Redshift Serverless Workgroup
- IAM Role for Redshift Spectrum
- Glue Database + Crawler
- S3 Path & Permissions

State files are excluded via `.gitignore`.

## ğŸ“ Project Structure

DOB-JOB-DE-PROJECT/
â”œâ”€â”€ dags/                         # Airflow DAGs and dbt project
â”‚   â”œâ”€â”€ dob_dbt_project/          # dbt transformations
â”‚   â”‚   â”œâ”€â”€ analyses/
â”‚   â”‚   â”œâ”€â”€ macros/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ seeds/
â”‚   â”‚   â”œâ”€â”€ snapshots/
â”‚   â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”‚   â”œâ”€â”€ profiles.yml
â”‚   â”‚   â””â”€â”€ .gitignore
â”‚   â””â”€â”€ full_pipeline_dag.py      # Airflow DAG to run the pipeline
â”‚
â”œâ”€â”€ job-dlt-pipeline/             # DLT ingestion pipeline
â”‚   â”œâ”€â”€ .dlt/
â”‚   â”‚   â”œâ”€â”€ config.toml    # (Should be gitignored)
â”‚   â”‚   â””â”€â”€ secrets.toml   # (Should be gitignored)
â”‚   â”œâ”€â”€ rest_api_pipeline.py
â”‚   â””â”€â”€ .gitignore
â”‚
â”œâ”€â”€ scripts/                      # Custom scripts for AWS, Redshift, etc.
â”‚   â”œâ”€â”€ copy_to_redshift.py
â”‚   â”œâ”€â”€ create_external_schema.py
â”‚   â”œâ”€â”€ metabase_automation.py
â”‚   â””â”€â”€ run_glue_crawler.py
â”‚
â”œâ”€â”€ terraform/                    # Infrastructure as Code (Terraform)
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â”œâ”€â”€ outputs.tf
â”‚   â”œâ”€â”€ terraform.tfvars          # (Should be gitignored)
â”‚   â”œâ”€â”€ terraform.tfstate         
â”‚   â”œâ”€â”€ .terraform/               # (Should be gitignored)
â”‚   â”œâ”€â”€ .terraform.lock.hcl
â”‚   â””â”€â”€ .gitignore
â”‚
â”œâ”€â”€ Dockerfile                    # Custom Airflow image
â”œâ”€â”€ docker-compose.yaml           # Run Airflow + Metabase
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ data_column_read.ipynb        # EDA notebook
â”œâ”€â”€ .env                          # Actual secrets (excluded from Git)
â”œâ”€â”€ .env.example                  # Example env file
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md                     # Project documentation
â””â”€â”€ Data-Dashboard/
    â””â”€â”€ nyc_dob_dashboard.png     # Image for README preview



---

## ğŸ’» Reproducibility

### 1. Clone this repo & setup env

```bash
git clone https://github.com/YOUR_USERNAME/nyc-dob-job-pipeline.git
cd nyc-dob-job-pipeline



Fill in:

- AWS keys
- Redshift credentials
- IAM role
- Metabase user
- dlt secret.toml

Run containers

- docker-compose up --build

âœ… Airflow: http://localhost:8080
âœ… Metabase: http://localhost:3000

Trigger DAG in Airflow to run full pipeline.

Bonus Improvements
âœ… Airflow DAG with clear dependencies
âœ… Automated Metabase dashboard creation
âœ… Modular repo with separate folders per tool
âœ… .gitignore for security


